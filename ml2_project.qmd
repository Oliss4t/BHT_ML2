---
title: "Machine Learning 2: Prediction the Individual Medical Cost"
author: "Conor Fallon; Dennis Fast; Leonhard Liu; Tassilo Henninger"
date: "17 1 2023"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    output-file: "ML2_report"
    output-ext:  "pdf"
editor_options: 
  markdown: 
    wrap: sentence
---

# Machine Learning 2: Prediction the Individual Medical Cost

\pagebreak

## Project Goal and Outline

<font style='color: #000000; background-color: #FF0000'>**Can add more as needed when all is done, is finished for now**</font>

The task at hand is a regression problem.
Broadly speaking, we wish to create a series of models, each of which will be able to make predictions on a withheld test test.
The dataset in question is a Medical Cost dataset.
The variable we wish to predict is the expected insurance premium for a given individual based on the following input variables:

-   age
-   sex: male or female
-   Body mass index, is a measure of one's weight relative to their height
-   children: Number of children/dependants also covered under this person's health insurance
-   smoker: Whether the person smokes or not
-   region: For locations: northeast, southeast, southwest, northwest. All refer to the United States
-   charges: The premium billed by the health insurance company. This is the predictor variable

Of course, some of these variables seem obvious in how they affect a person's insurance premium; a 98 year-old obese smoker will have a higher insurance premium than a 24 year-old non-smoker with a healthy BMI.
However, through some exploratory data analysis as well as by comparing our methods, a more discerning picture of how each variable impacts the value we wish to predict will be discussed.

There will be three models compared to each other: a baseline model, which consists of a simple linear regression; a regression tree; and a neural network.
The best model is ultimately decided upon by using the Root Mean Squared error, although other metrics, where suitable, will be used to influence our decision making.

For both the Neural Network and the Regression Tree models, we wish to find a 'best' Neural Network and a 'best' Regression Tree with which to work.
For this purpose, these models are initially trained only on the train split (as outlined further on), and the 'best' model for each is decided upon based on their performance on a validation split.
As such, the selection for the best model in our two algorithm classes is based on the validation error.

It is important to emphasise that this leaves the test data untouched at the stage when these models are created, leaving the test data purely for the final model comparison between the best Neural Net and the best Regression Tree.
Then when ultimately comparing the two chosen models, these are retrained on the combined train and validation splits, and finally tested on the test set, which has been withheld for this purpose.

This report will conclude with a discussion of our findings, an examination of what worked and what did not work, and will also mention avenues for future work.

## Dateset and Preprocessing

```{r message=FALSE, warning=FALSE, include=FALSE}
## Load libraries
library(dlookr)
library(corrplot)
library("PerformanceAnalytics")
library(splitTools)
library(ranger)
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
```

```{r,, include=FALSE , include=FALSE}
## Load data

df.insurance <- read.csv(file ='./data/insurance.csv', header = TRUE)
describe(df.insurance)
```

-   a short description of your data

Dataset:

-   https://github.com/stedy/Machine-Learning-with-R-datasets
-   https://www.kaggle.com/datasets/mirichoi0218/insurance

The dataset is included in a Book called "Machine Learning with R" by Brett Lantz and is also available on Kaggle.
The dataset contains 1338 rows, 7 columns and there are no missing values.
The 6 predictor variables are: age(numeric,integer), sex(female/male), bmi(numeric,float), children(numeric,integer), smoker(boolean) and region(categorical).
The outcome variable is "charges" (numeric,float).
Thereby it is a regression problem and the goal is to accurately predict the insurance costs.

<font style='color: #000000; background-color: #FF0000'>**check for missing values**</font>

```{r}
num.cols <- unlist(lapply(data, is.numeric))
num.cols <- c('age', 'bmi', 'children', 'charges')
cat.cols <- c('sex', 'smoker', 'region')
df.insurance <- df.insurance %>%
  mutate(sex = factor(sex, levels = unique(df.insurance$sex))) %>%
  mutate(smoker = factor(smoker, levels = unique(df.insurance$smoker))) %>%
  mutate(region = factor(region, levels = unique(df.insurance$region)))
```

-   Split dataset into 60% train set, 20% validation and 20% test set and save the data for later use in Python.

```{r}

set.seed(1)
inds <- partition(df.insurance$charges, p = c(train = 0.6, valid = 0.2, test = 0.2))
str(inds)

#$ train: int [1:804] 2 3 7 8 9 11 12 15 16 18 ...
#$ valid: int [1:266] 5 17 19 31 36 41 46 47 49 51 ...
#$ test : int [1:268] 1 4 6 10 13 14 20 22 24 26 ...

train <- df.insurance[inds$train, ]
valid <- df.insurance[inds$valid, ]
test <- df.insurance[inds$test, ]

# write.csv(train, './data/train.csv', row.names=TRUE)
# write.csv(valid, './data/val.csv', row.names=TRUE)
# write.csv(test, './data/test.csv', row.names=TRUE)

train <- read.csv(file ='./data/train.csv', header = TRUE)
valid <- read.csv(file ='./data/valid.csv', header = TRUE)
test <- read.csv(file ='./data/test.csv', header = TRUE)

```

## EDA

One of the objectives of preliminary data analysis to get a feel for the data you are dealing with by describing the key features of the data and summarizing the results.

-   Plots correlation netween numerical features

On the correlation matrix plot we see, that Charges has the strongest correlation with Age (0.3) and Children only 0.068.
For the correlations between the explanatory variables the following stand out:

<font style='color: #000000; background-color: #FF0000'>**include sex, smoker and region as well.**</font>

-   0.938: Economy and Internet ...

```{r}
chart.Correlation(df.insurance[num.cols], histogram = TRUE, pch = 19)
```

-   Plots correlation netween numerical features

```{r}
ggplot(df.insurance, aes(x = age, y = charges, fill = smoker, size = bmi, alpha = .3)) +
  geom_point(pch = 21) +
  scale_size_continuous(range = c(0, 10))

```

# Forecasting

## Baseline - Linear Regression

Create simple linear model with 'charges' as outcome variable and inclusing all other columns as predictor variables.

### Mathematical Overview

-   explanation what the model does, including mathematical notation

### Hyperparameter Optimization

-   a description of your fitting process including, a summary of how you arrived at your final model, the choice of hyperparameters and how you made this choice,

```{r}

lm1 <- lm(charges ~ ., train)
summary(lm1)

```

-   Predict the outcome of the validation set and calculate RMSE and R2 scores.

```{r}
lm1.pred <- predict(lm1, valid, se.fit = TRUE)
RMSE(lm1.pred$fit, valid$charges) # [1] 5914.46
R2(lm1.pred$fit, valid$charges) # [1] 0.7619868
```

### Performance

-   an appropriate assessment of the predicted values and

-   Predict the outcome of the test set and calculate RMSE and R2 scores.

```{r}
lm1.pred <- predict(lm1, test, se.fit = TRUE)
RMSE(lm1.pred$fit, test$charges)
# [1] 5670.218
R2(lm1.pred$fit, test$charges)
# 0.773166
```

## Algorithm 1 - Tree Models

### Mathematical Overview

-   explanation what the model does, including mathematical notation

A regression tree is a specific class of tree which will predict a numerical dependent variable based on a number of explanatory variables.
They can deal with the situation in which the explanatory variables interact with one another (BMI and age are in all likelihood in someway dependent).
This can be difficult to model through simpler models like linear regression; however, by partitioning the data into smaller regions, the interactions between the explanatory variables, potentially very complex, can become more manageable.

In a tree model, this partitioning and sub-partitioning (i.e. recursively partitioning each partition) is done by the tree.

We want a set of predictor variables $X_1, ..., X_P$, into some number $J$ distinct and mutually exclusive regions (partitions) $R_1, ..., R_J$.
In this description, each observation that falls into the region $R_j$ will have the same value assigned to it.

These regions are decided upon as follows: we want to find a selection of regions $R_1, ..., R_J$ such that the residual squared error (RSS) is minimised.
I.e. minimise

$\Sigma_{j=1}^J$ $\Sigma_{i \in R_j}$ $(y_i - \hat{y}_{R_j})^2$

where $\hat{y}_{R_j}$ is the mean value for the predictor variable in that given region based on the training data.

The splitting is done in a top-down, greedy manner, where the splitting starts at the top of the tree (i.e. all data-points belong to the same region), then this split in 2, and this process is repeated for each split in the tree.
It is greedy in the sense that it does not look ahead steps into the future; it simply predicts the best split for the split in question.

The actual splitting is done as follows.
A predictor variable $X_j$ and a splitting point $s$ such that the splitting regions $X|X_j < s$ and $X|X_j \ge s$ provides the smallest possible RSS.
This is then repeated for the other predictor variables until the predictor variable which gave had the value of $s$ which gave the smallest RSS is found and this is chosen as the variable for this split in the tree.
We do this until a stopping criteria is found, one which will ideally neither underfit nor overfit the tree.
In our case, we are examining the complexity parameter in order to do this.

Of course, if we completely overfit the tree, there will be the smallest RSS; however, we want to penalise overfitting in some way.
The approach used here is penalised least squares (PLS).
Let us call our full tree $T_0$.
We want to find a pruned tree $T_\alpha$ $\subset$ $T_0$ such that

$PLS(\alpha) = \Sigma_{j \in |T_\alpha|}^J$ $\Sigma_{i \in R_j}$$(y_i - \hat{y}_{R_j})^2 + \alpha |T_\alpha|$

with $\alpha$ greater than or equal to zero, and the other variables as defined already.
$\alpha$ is the complexity parameter.
It can be found using $K$-fold cross validation.
The rule of thumb that is used by the rpart package is to take the largest value of the complexity parameter that is within 1 standard deviation of the smallest value out of the cross validation setup.
### Hyperparameter Optimization

-   a description of your fitting process including, a summary of how you arrived at your final model, the choice of hyperparameters and how you made this choice,

-   regression tree out of the box: standard hyperparameter settings, baseline of the regression trees.
    Uses 10-fold cross validation as its default to find the complexity parameter.

```{r, echo=FALSE}
tree.data<-rpart(charges~.,train)
print(tree.data)
```

Here we will follow the tactic of creating a full tree (i.e. totally overfitted) which will then be pruned back.
We can see in the below plot how the error decreases as the complexity parameter decreases.
Based on this plot a suitable complexity parameter is selected, and this quite interpretable decision tree is shown.
Note, there exist other error measures for the splitting criteria and this will be discussed further on.

-   completly overfitte regression tree

```{r, echo=FALSE}

tree.data.full<-rpart(charges~.,train,cp=0)

cpmatrix<-printcp(tree.data.full)
plotcp(tree.data.full)

prune.data=prune(tree.data,cp=0.019)
rpart.plot(prune.data)
```

Let us predict and get some metrics as was done for the baseline model.
Do this for the validation set.
This is initially done on the full, overfitted tree, then on the default one, and then on the one with the selected complexity parameter

```{r}
pred.train.full <- predict(tree.data.full, valid, se.fit = TRUE)

#Validation data
RMSE(pred.train.full, valid$charges) 
R2(pred.train.full, valid$charges) 


#exclude that part!
#Test data
pred.train.full <- predict(tree.data.full, test, se.fit = TRUE)
RMSE(pred.train.full, test$charges) 
R2(pred.train.full, test$charges) 
```

Repeat for the other two models Default:

```{r}
# out of the box tree model
pred.train.default <- predict(tree.data, valid, se.fit = TRUE)
RMSE(pred.train.default, valid$charges) 
R2(pred.train.default, valid$charges) 
```

```{r}
pred.train.default <- predict(tree.data, test, se.fit = TRUE)
RMSE(pred.train.default, test$charges) 
R2(pred.train.default, test$charges) 
```

Pruned:

```{r}
# Chosen complexity parameter based on the theory and graph
pred.train.pruned <- predict(prune.data, valid, se.fit = TRUE)
RMSE(pred.train.pruned, valid$charges) 
R2(pred.train.pruned, valid$charges) 
```

```{r}
pred.train.pruned <- predict(prune.data, test, se.fit = TRUE)
RMSE(pred.train.pruned, test$charges) 
R2(pred.train.pruned, test$charges) 
```

<font style='color: #000000; background-color: #FF0000'>**Do the Gridsearch for the splitting criteria as well, gini index vs. entropy**</font>

Finding the optimal complexity parameter via grid search:

```{r}
param_grid <- expand.grid(cp = seq(0.01, 0.5, 0.01))

regtree <- train(
  charges ~ .,
  data = train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = param_grid
)

print(regtree$bestTune)

predictions <- predict(regtree, newdata = valid)

RMSE(predictions, valid$charges) 
R2(predictions, valid$charges) 
```

### Performance

-   an appropriate assessment of the predicted values and <font style='color: #000000; background-color: #FF0000'>**Include variable importance. For a singel tree you can interpret a fitted model by inspecting the text output of the tree or the tree diagram itself. We could also calculate the Increase in Node Purity and Mean decrease in Accuracy, but that is most often done for ensembled methods, as you e.g can not plot all trees of a random forest.**</font>

The best result was for the final tree found via hyperparameter optimisation which gave an RMSE of \[1\] 4759.202 and an R2 of \[1\] 0.8432418 on the validation data.
It looks as follows

```{r}
printcp(rpart(charges~.,train,cp=0.01))
plotcp(rpart(charges~.,train,cp=0.01))
rpart.plot(rpart(charges~.,train,cp=0.01))
```

## Algorithm 2 - Neural Networks

### Mathematical Overview

-   explanation what the model does, including mathematical notation

### Hyperparameter Optimization

-   a description of your fitting process including, a summary of how you arrived at your final model, the choice of hyperparameters and how you made this choice,

### Performance

-   an appropriate assessment of the predicted values and

## Model Comparrison

-   fair comparison of the two methods using the test data

```{r}
pred.regtree.test <- predict(regtree, newdata = test)

RMSE(pred.regtree.test, test$charges)
R2(pred.regtree.test, test$charges)
```

```{r}
pred.nn.test <- read.csv(file ='./results/predictions_nn.csv', sep = ' ', header = FALSE)
pred.nn.test <- t(pred.nn.test)
pred.nn.test <- head(pred.nn.test, - 1)

RMSE(pred.nn.test, test$charges) 
R2(pred.nn.test, test$charges) 
```

# Future Work and Discussion
