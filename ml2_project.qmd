---
title: "Machine Learning 2: Prediction the Individual Medical Cost"
author: "Conor Fallon; Dennis Fast; Leonhard Liu; Tassilo Henninger"
date: "17 1 2023"
format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    output-file: "ML2_report.pdf"
    output-ext:  "pdf"
editor_options: 
  markdown: 
    wrap: sentence
---

# Machine Learning 2: Prediction the Individual Medical Cost

\pagebreak

## Project Goal and Outline

<font style='color: #000000; background-color: #FF0000'>**Can add more as needed when all is done, is finished for now**</font>

The task at hand is a regression problem.
Broadly speaking, we wish to create a series of models, each of which will be able to make predictions on a withheld test test.
The dataset in question is a Medical Cost dataset.
The variable we wish to predict is the expected insurance premium for a given individual based on the following input variables:

-   age
-   sex: male or female
-   Body mass index, is a measure of one's weight relative to their height
-   children: Number of children/dependants also covered under this person's health insurance
-   smoker: Whether the person smokes or not
-   region: For locations: northeast, southeast, southwest, northwest. All refer to the United States
-   charges: The premium billed by the health insurance company. This is the predictor variable

Of course, some of these variables seem obvious in how they affect a person's insurance premium; a 98 year-old obese smoker will have a higher insurance premium than a 24 year-old non-smoker with a healthy BMI.
However, through some exploratory data analysis as well as by comparing our methods, a more discerning picture of how each variable impacts the value we wish to predict will be discussed.

There will be three models compared to each other: a baseline model, which consists of a simple linear regression; a regression tree; and a neural network.
The best model is ultimately decided upon by using the Root Mean Squared error, although other metrics, where suitable, will be used to influence our decision making.

For both the Neural Network and the Regression Tree models, we wish to find a 'best' Neural Network and a 'best' Regression Tree with which to work.
For this purpose, these models are initially trained only on the train split (as outlined further on), and the 'best' model for each is decided upon based on their performance on a validation split.
As such, the selection for the best model in our two algorithm classes is based on the validation error.

It is important to emphasise that this leaves the test data untouched at the stage when these models are created, leaving the test data purely for the final model comparison between the best Neural Net and the best Regression Tree.
Then when ultimately comparing the two chosen models, these are retrained on the combined train and validation splits, and finally tested on the test set, which has been withheld for this purpose.

This report will conclude with a discussion of our findings, an examination of what worked and what did not work, and will also mention avenues for future work.

## Dateset and Preprocessing

```{r message=FALSE, warning=FALSE, include=FALSE}
## Load libraries
library(dlookr)
library(corrplot)
library("PerformanceAnalytics")
library(splitTools)
library(ranger)
library(tidyverse)
library(caret)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(dplyr)
library(gridExtra)
library(GGally)
library(PMCMRplus)
library("corrplot")
```

```{r include=FALSE}
## Load data

df.insurance <- read.csv(file ='./data/insurance.csv', header = TRUE)
describe(df.insurance)
```

The dataset we used is included in a Book called "Machine Learning with R" by Brett Lantz and is also available on Kaggle. It can be downloaded via the following links:

-   https://github.com/stedy/Machine-Learning-with-R-datasets
-   https://www.kaggle.com/datasets/mirichoi0218/insurance

It contains 1338 rows, 7 columns and there are no missing values. The 6 predictor variables are like in the project outline introduced: age(numeric, integer), sex(female / male), BMI(numeric, float), children(numeric, integer), smoker(boolean) and region(categorical). The outcome variable is "charges" (numeric, float).

The only preprocessing we had to do was encoding the categorical features as factor variables. Next we split our dataset into 60% train set, 20% validation and 20% test set, like it is ask in the project description.

```{r}
num.cols <- unlist(lapply(data, is.numeric))
num.cols <- c('age', 'bmi', 'children', 'charges')
cat.cols <- c('sex', 'smoker', 'region')
df.insurance <- df.insurance %>%
  mutate(sex = factor(sex, levels = unique(df.insurance$sex))) %>%
  mutate(smoker = factor(smoker, levels = unique(df.insurance$smoker))) %>%
  mutate(region = factor(region, levels = unique(df.insurance$region)))
```



```{r}

set.seed(1)
inds <- partition(df.insurance$charges, p = c(train = 0.6, valid = 0.2, test = 0.2))
str(inds)

#$ train: int [1:804] 2 3 7 8 9 11 12 15 16 18 ...
#$ valid: int [1:266] 5 17 19 31 36 41 46 47 49 51 ...
#$ test : int [1:268] 1 4 6 10 13 14 20 22 24 26 ...

train <- df.insurance[inds$train, ]
valid <- df.insurance[inds$valid, ]
test <- df.insurance[inds$test, ]

# write.csv(train, './data/train.csv', row.names=TRUE)
# write.csv(valid, './data/val.csv', row.names=TRUE)
# write.csv(test, './data/test.csv', row.names=TRUE)

train <- read.csv(file ='./data/train.csv', header = TRUE)
valid <- read.csv(file ='./data/valid.csv', header = TRUE)
test <- read.csv(file ='./data/test.csv', header = TRUE)

```

## EDA

One of the objectives of exploratory data analysis is to get a feel for the data you are dealing with by describing the key features of the data and summarizing the results. The first thing we can do is to take a look at the descriptive statistics via the summary function and the histograms. We have individuals ranging from the age of 18 to 64 with a mean and median around 39 ages. The sex is almost perfectly balanced as 49.5% of the individuals are female and 50.5% are male.The bmi is ranging from 15.96 to 53.13 with a mean and median around 30. The bmi is calculated by the following formula:  $$BMI = \frac{mass_{kg}}{{height_m}^2}$$
A common use of the BMI is to assess how far an individual's body weight departs from what is normal for a person's height. The WHO regards an adult BMI of less than 18.5 as underweight, a BMI of 25 or more is as overweight and 30 or more is considered obese. From the summary and the histogram we can clearly see, that our population of individuals is in mean considered obese and therefore an unhealthy population.
Most of the individuals have no children, the mean is 1 child and the maximum is 5. Regarding the factor feature smoker the dataset is unbalanced. Only 20.5% of our individuals are smoker, respectively 79.5% are non smoker. The region is good balanced again between the four categories "southwest","southeast","northwest" and "northeast".

Last we have our predictor variable charges. The minimum of a health insurance charge is 1122\$, the median is 9382\$, the mean 13270\$ and the maximum 63770\$. From the difference of median and mean we can already tell that the distribution is skewed to the right (positively skewed). This means that there are a few large values that are pulling the mean higher, but the majority of the data is concentrated around the median. 
Looking at the boxplot for the charge we can confirm that. We can inspect a long tail of high values and a shorter tail of low values. We have no symmetric distribution.

```{r}
summary(df.insurance)
```

```{r, echo=FALSE,fig.width=10,fig.height=8}

plot_histograms <- function(df) {
  plots <- list()
  for (col in names(df)) {
    if(is.numeric(df[,col])){
      plots[[col]] <- ggplot(df, aes_string(col)) + 
        geom_histogram(color="black", fill="white", binwidth = (max(df[,col], na.rm = TRUE)-min(df[,col], na.rm = TRUE))/30) + 
        ggtitle(paste("Histogram of", col)) 
    } else if(is.factor(df[,col])){
      plots[[col]] <- ggplot(df, aes_string(col)) + 
        geom_bar(color="black", fill="white") +
        ggtitle(paste("Histogram of", col)) 
    }
  }
  grid.arrange(grobs=plots, ncol=3,  widths = rep(3,3), heights = rep(3,3))
}
plot_histograms(df.insurance)
```

Next, we want to better understand the relationships between our different variables in the dataset. Therefore we use the correlation matrix to get insights about our data regarding the 
linearity and their strength. A positive correlation means that as one variable increases, the other variable also increases, and a negative correlation means that as one variable increases, the other variable decreases. A correlation value of 1 or -1 indicates a perfect linear relationship between the variables. A correlation value of 0 indicates that there is no linear relationship between the variables. 
Note however, that correlation does not imply causality, it just indicates that two variables are related. That means the correlation between two variables may be high, but may be that the correlation is due to a relationship between the two variables and a third variable. This concept is called multicollinearity.
This has to be kept in mind for our baseline model. If we create a linear model and want to interpret the coefficients on how they influence our charge, multicollinearity can be a problem, as interpretability assumes that you can only change the value of one explanatory variable and not the others at the same time. This of course is only true if there are no correlations between the explanatory variables. If this independence does not hold, we have a problem of multicollinearity. Multicollinearity can result in the coefficients swinging wildly based on which other independent variables are in the model. Therefore the coefficients become very sensitive to small changes in the model and can not be easily interpreted. One way to detect multicollinearity is to examine the correlation matrix and look for high correlation coefficients between the multiple explanatory variables.

As the correlation metric can only be calculate for numeric values, we get the correlation scores for the features age, bmi, children and charges. Charges has the strongest correlation with Age (+0.299), then bmi (+0.198) and last children with only +0.068. The highest correlation between explanatory variables is +0.109 between bmi and age. As this quite low, we dont have a problem of multicollinearity betweeen the numeric explanatory variables.

```{r,echo=FALSE, message=FALSE,fig.width=10,fig.height=8}
plot_cor_matrix <- ggpairs(df.insurance, title="Correlation Matrix", )
plot_cor_matrix
```

One way to assess the influence of categorical variables on an outcome variable is through a one-way ANOVA, which compares the means of a numerical outcome variable across different levels of a categorical variable. Thereby we could determine if there is a significant difference in the means of the outcome variable between the different levels of the categorical variable. 
However, the ANOVA assumes that the data is normally distributed, independent and that the variances of the groups are equal. We already found that outcome "charges" is positively skewed and not normally distributed. Thereby, the assumption of the ANOVA fails. We can however use the non-parametric alternatives called Kruskal-Wallis test.

The output of Kruskal-Wallis test statistics gives us the p-value, though which we can determine the significance of the test. A small p-value (typically less than 0.05) indicates strong evidence against the null hypothesis, which in this case is that there is no difference in the medians of the outcome variable between the different levels of the categorical variable. As only the p-value of the categorical variable "smoker" is below the significance level of 5%, we can only reject the null hypothesis for smoker and therefore say, there is a significant difference in the medians of the outcome variable between beeing a smoker and not. We can confirm the result also by looking at the boxplot, splitted up by category. This is also visible in small boxplots in the correlation matrix. 

```{r, warning=FALSE, message=FALSE }
kruskal.test(df.insurance$charge , df.insurance$sex)
kruskal.test(df.insurance$charge , df.insurance$smoker)
kruskal.test(df.insurance$charge , df.insurance$region)

```

```{r, echo=FALSE,fig.width=9,fig.height=6}
plot_sex <- df.insurance %>% 
    ggplot(aes(x = sex, y = charges)) +
    geom_boxplot() +
    geom_jitter(alpha = 0.2) +
    xlab("Boxplot of Sex") +
    theme_classic()
plot_smoker <- df.insurance %>% 
    ggplot(aes(x = smoker, y = charges)) +
    geom_boxplot() +
    geom_jitter(alpha = 0.2) +
    xlab("Boxplot of Smoker") +
    theme_classic()
plot_region <- df.insurance %>% 
    ggplot(aes(x = region, y = charges)) +
    geom_boxplot() +
    geom_jitter(alpha = 0.2) +
    xlab("Boxplot of Region") +
    theme_classic()
grid.arrange(plot_sex,plot_smoker,plot_region, ncol=3 )

```

# Forecasting

## Baseline - Linear Regression

In order to check the performance of the chosen models, we first created simple linear regression model as a baseline model, with 'charges' as outcome variable and including all other columns ('age', 'sex', 'BMI', 'children', 'smoker', and 'region') as predictor variables.

### Mathematical Overview

The mathematical notation of a linear model represents a linear relationship between the outcome variable 'charges' and the predictor variables 'age', 'sex', 'BMI', 'children', 'smoker', and 'region'.
The equation states that the outcome variable 'charges' is a linear combination of the predictor variables, with each predictor variable multiplied by a specific coefficient (beta):

$charges = \beta_{0} + \beta_{1}*age + \beta_{2}*sex + \beta_{3}*BMI + \beta_{4}*children + \beta_{5}*smoker + \beta_{6}*region$

In this equation, 'charges' is represented as charges, the coefficients are represented as $\beta_{0}$, $\beta_{1}$, $\beta_{2}$, $\beta_{3}$, $\beta_{4}$, $\beta_{5}$, $\beta_{6}$ and predictor variables 'age', 'sex', 'BMI', 'children', 'smoker', and 'region' are represented as age, sex, BMI, children, smoker, region respectively.


The equation can be used to make predictions about the outcome variable 'charges' based on the values of the predictor variables.
For example, if we know the values of 'age', 'sex', 'BMI', 'children', 'smoker', and 'region' for a specific individual, we can plug those values into the equation and solve for 'charges'.
The result of this calculation would be an estimate of the individual's 'charges'. 

If our linear model has good predictability, we can also interpret the coefficients on how they influence the outcome. As we found no strong multicollinearity during EDA we can do that in the following. This is called regression analysis.


### Hyperparameter Optimization

Since we use the linear model as a baseline, we don't conduct the hyperparameter optimization on the model.

```{r}
lm1 <- lm(charges ~ ., train)
summary(lm1)
```



### Performance

Finally, we predict the outcome on the validation set and calculate RMSE and R2 scores.

The R2 score of 0.71 is good enough for interpreting the coefficients. From the model summary we can see hat the followin explanatory variables are statistically significant:

-   Age has an positive effect on the charges. One unit change on age results in an increase of the charges of **259.98$**
-   BMI has an positive effect on the charges. One unit change on BMI results in an increase of the charges of **327.40$**
-   Children has an positive effect on the charges. One unit change on Children results in an increase of the charges of **477.80$**
-   BMI has an positive effect on the charges. One unit change on BMI results in an increase of the charges of **327.40$**
-   Smoker has an positive effect on the charges. Being a smoker results in an increase of the charges of **23594.95$**


```{r}
pred.lm.test <- predict(lm1, test, se.fit = TRUE)
RMSE(pred.lm.test$fit, test$charges)
R2(pred.lm.test$fit, test$charges)
```

## Algorithm 1 - Tree Models

### Mathematical Overview

-   explanation what the model does, including mathematical notation

A regression tree is a specific class of tree which will predict a numerical dependent variable based on a number of explanatory variables.
They can deal with the situation in which the explanatory variables interact with one another (BMI and age are in all likelihood in someway dependent).
This can be difficult to model through simpler models like linear regression; however, by partitioning the data into smaller regions, the interactions between the explanatory variables, potentially very complex, can become more manageable.

In a tree model, this partitioning and sub-partitioning (i.e. recursively partitioning each partition) is done by the tree.

We want a set of predictor variables $X_1, ..., X_P$, into some number $J$ distinct and mutually exclusive regions (partitions) $R_1, ..., R_J$.
In this description, each observation that falls into the region $R_j$ will have the same value assigned to it.

These regions are decided upon as follows: we want to find a selection of regions $R_1, ..., R_J$ such that the residual squared error (RSS) is minimised.
I.e. minimise

$\Sigma_{j=1}^J$ $\Sigma_{i \in R_j}$ $(y_i - \hat{y}_{R_j})^2$

where $\hat{y}_{R_j}$ is the mean value for the predictor variable in that given region based on the training data.

The splitting is done in a top-down, greedy manner, where the splitting starts at the top of the tree (i.e. all data-points belong to the same region), then this split in 2, and this process is repeated for each split in the tree.
It is greedy in the sense that it does not look ahead steps into the future; it simply predicts the best split for the split in question.

The actual splitting is done as follows.
A predictor variable $X_j$ and a splitting point $s$ such that the splitting regions $X|X_j < s$ and $X|X_j \ge s$ provides the smallest possible RSS.
This is then repeated for the other predictor variables until the predictor variable which gave had the value of $s$ which gave the smallest RSS is found and this is chosen as the variable for this split in the tree.
We do this until a stopping criteria is found, one which will ideally neither underfit nor overfit the tree.
In our case, we are examining the complexity parameter in order to do this.

Of course, if we completely overfit the tree, there will be the smallest RSS; however, we want to penalise overfitting in some way.
The approach used here is penalised least squares (PLS).
Let us call our full tree $T_0$.
We want to find a pruned tree $T_\alpha$ $\subset$ $T_0$ such that

$PLS(\alpha) = \Sigma_{j \in |T_\alpha|}^J$ $\Sigma_{i \in R_j}$$(y_i - \hat{y}_{R_j})^2 + \alpha |T_\alpha|$

with $\alpha$ greater than or equal to zero, and the other variables as defined already.
$\alpha$ is the complexity parameter.
It can be found using $K$-fold cross validation.
The rule of thumb that is used by the rpart package is to take the largest value of the complexity parameter that is within 1 standard deviation of the smallest value out of the cross validation setup.
### Hyperparameter Optimization

-   a description of your fitting process including, a summary of how you arrived at your final model, the choice of hyperparameters and how you made this choice,

This is how the rpart regression tree works out of the box: the standard hyperparameter settings form a baseline of the regression trees.
Rpart uses 10-fold cross validation as its default to find the complexity parameter.

```{r, echo=FALSE}
tree.data<-rpart(charges~.,train)
print(tree.data)
```

The next tree to be formed will be a full tree (i.e. totally overfitted) with complexity parameter equaling 0, which will then be pruned back.
We can see in the below plot how the error decreases as the complexity parameter decreases.
Based on this plot a suitable complexity parameter is selected, and this quite interpretable decision tree is shown.
Note, there exist other error measures for the splitting criteria and this will be discussed further on.

-   completly overfitte regression tree

```{r, echo=FALSE}

tree.data.full<-rpart(charges~.,train,cp=0)

cpmatrix<-printcp(tree.data.full)
plotcp(tree.data.full)

prune.data=prune(tree.data,cp=0.019)
rpart.plot(prune.data)
```

Let us predict and get some metrics as was done for the baseline model.
Do this for the validation set.
This is initially done on the full, overfitted tree, then on the default one, and then on the one with the selected complexity parameter

```{r}
pred.train.full <- predict(tree.data.full, valid, se.fit = TRUE)

#Validation data
RMSE(pred.train.full, valid$charges) 
R2(pred.train.full, valid$charges) 

```

Repeat for the other two models Default:

```{r}
# out of the box tree model
pred.train.default <- predict(tree.data, valid, se.fit = TRUE)
RMSE(pred.train.default, valid$charges) 
R2(pred.train.default, valid$charges) 
```

Pruned:

```{r}
# Chosen complexity parameter based on the theory and graph
pred.train.pruned <- predict(prune.data, valid, se.fit = TRUE)
RMSE(pred.train.pruned, valid$charges) 
R2(pred.train.pruned, valid$charges) 
```

<font style='color: #000000; background-color: #FF0000'>**Do the Gridsearch for the splitting criteria as well, gini index vs. entropy**</font>

Finding the optimal complexity parameter via grid search:

```{r}
param_grid <- expand.grid(cp = seq(0.01, 0.5, 0.01))

regtree <- train(
  charges ~ .,
  data = train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = param_grid
)

print(regtree$bestTune)

predictions <- predict(regtree, newdata = valid)

RMSE(predictions, valid$charges) 
R2(predictions, valid$charges) 
```

### Performance

-   an appropriate assessment of the predicted values and <font style='color: #000000; background-color: #FF0000'>**Include variable importance. For a singel tree you can interpret a fitted model by inspecting the text output of the tree or the tree diagram itself. We could also calculate the Increase in Node Purity and Mean decrease in Accuracy, but that is most often done for ensembled methods, as you e.g can not plot all trees of a random forest.**</font>

The best result was for the final tree found via hyperparameter optimisation which gave an RMSE of \[1\] 4759.202 and an R2 of \[1\] 0.8432418 on the validation data.
It looks as follows

```{r}
printcp(rpart(charges~.,train,cp=0.01))
plotcp(rpart(charges~.,train,cp=0.01))
rpart.plot(rpart(charges~.,train,cp=0.01))
```

## Algorithm 2 - Neural Networks

### Mathematical Overview

A neural network is a type of machine learning model that is inspired by the structure and function of the human brain.
It is composed of layers of interconnected nodes, or "neurons," that process and transmit information.
Neural networks are capable of handling complex and non-linear relationships between input and output variables.

A neural network can be represented mathematically as a series of layers, with each layer performing a set of mathematical operations on the input it receives.
The output of one layer serves as the input to the next layer.
The first layer, called the input layer, receives the input variables, and the last layer, called the output layer, produces the output variable(s).
In between the input and output layers, there are one or more hidden layers that perform computations on the input data.

The mathematical notation for a neural network can be quite complex as it depends on the architecture of the network, the number of layers and the activation functions used.
In general, a neural network can be represented mathematically as:

$y = f(W_n * f(W_{n-1} * f(...f(W_1 * x + b_1) + b_2)... + b_n) + b_{n+1})$

Where:

-   y = output variable
-   x = input variable
-   f = activation function (such as sigmoid, ReLU, etc.)
-   W = weight matrix
-   b = bias term

This equation is showing a simplified version of a feedforward neural network with one hidden layer, the input, hidden and output layers are represented by x, f(W1x + b1), f(Wnf(W\_{n-1} \* f(...f(W_1 \* x + b_1) + b_2)... + b_n) + b\_{n+1}) respectively.

### Implementation

#### Python libraries

To train the neural network model, we use Python and the following libraries:

-   **pandas** and **numpy** for wrangling data

-   **sklearn** for preprocessing data

-   **torch** for defining the neural network model

-   **pytorch_lightning** for running and logging the training

-   **optuna** for hyperparameter optimization

```{python eval=FALSE}
import pandas as pd
import numpy as np

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import SGD, Adam

from pytorch_lightning.loggers import TensorBoardLogger, logger
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.callbacks.early_stopping import EarlyStopping

import optuna
```

#### Data preprocessing

To preprocess the data, we transformed the categorical variables using **LabelEncoder** and scaled the numerical data using **MinMaxScaler** from **sklearn** library:

```{python eval=FALSE}
def labelEncoder(df):
    for col in df.columns:
        if df.dtypes[col] == "object":
            df[col] = LabelEncoder().fit_transform(df[col])
            df[col] = df[col].astype('category')
    return df

cols_to_scale = ['age', 'bmi', 'children']
scaler = MinMaxScaler()
train[cols_to_scale] = scaler.fit_transform(train[cols_to_scale])
val[cols_to_scale] = scaler.transform(val[cols_to_scale])
test[cols_to_scale] = scaler.transform(test[cols_to_scale])
```

#### Dataset Architecture

To run the training process, we have to define custom data classes first, **trainDataset**, **valDataset**, and **testDataset** that are used to load and preprocess the data for training, validation and testing respectively.

Each dataset class takes one parameter, which is a DataFrame **df** containing the data, and creates two attributes, **X** and **y** from the dataframe.
**X** is a tensor containing the input features 'age', 'sex', 'bmi', 'children', 'smoker', 'region' and **y** is a tensor containing the target variable 'charges'.

The **len** method returns the number of data points in the dataset, and the **getitem** method is used to retrieve a specific data point at a given index.

```{python eval=FALSE}
class trainDataset(torch.utils.data.Dataset):
    def __init__(self, df):
        self.X = torch.tensor(df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']].values, dtype=torch.float32)
        self.y = torch.tensor(df['charges'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class valDataset(torch.utils.data.Dataset):
    def __init__(self, df):
        self.X = torch.tensor(df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']].values, dtype=torch.float32)
        self.y = torch.tensor(df['charges'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

class testDataset(torch.utils.data.Dataset):
    def __init__(self, df):
        self.X = torch.tensor(df[['age', 'sex', 'bmi', 'children', 'smoker', 'region']].values, dtype=torch.float32)
        self.y = torch.tensor(df['charges'].values, dtype=torch.float32)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

#### Model Architecture

The following section describes the model architecture of the neural network model.
The **RegressionModel** class is a subclass of PyTorch's LightningModule, which is a high-level wrapper for PyTorch's neural network module.

The class takes four parameters:

-   layers: an integer representing the number of hidden layers in the network

-   num_neurons: an integer representing the number of neurons in each hidden layer

-   learning_rate: a float representing the learning rate for the optimizer

-   batch_size: an integer representing the batch size to use during training

The **init** method is called when the class is first instantiated and sets up the neural network architecture.
It creates a series of hidden layers using the helper function hidden_layer.
The input layer of the network is defined with 6 input features, and the output layer has 1 output feature.
Each hidden layer is defined as a linear layer followed by a ReLU activation function.

The **forward** method defines the computation that takes place within the neural network, where input data is passed through the layers and transformed into output predictions.

The **training_step**, **validation_step**, and **test_step** methods define what happens during each step of the training, validation, and test processes respectively.
They calculate the mean squared error loss between the predictions and the true values, log the loss and return it for optimization.

The **configure_optimizers** method sets the optimizer to use for training, in this case, the **Adam** optimizer with the specified learning rate.

The **train_dataloader**, **val_dataloader** and **test_dataloader** methods return the data loaders for training, validation, and test dataset respectively.

```{python eval=FALSE}
def hidden_layer(in_f, out_f):
    return torch.nn.Sequential(
        torch.nn.Linear(in_f, out_f),
        torch.nn.ReLU()
    )

class RegressionModel(pl.LightningModule):
    def __init__(self, layers, num_neurons, learning_rate, batch_size):
        super().__init__()
        self.learning_rate = learning_rate
        self.batch_size = batch_size
        self.num_neurons = num_neurons
        hidden = [hidden_layer(num_neurons, num_neurons) for _ in range(layers)]              
        self.input = torch.nn.Linear(6, num_neurons)
        self.hidden = torch.nn.Sequential(*hidden)
        self.output = torch.nn.Linear(num_neurons, 1)
        self.relu = torch.nn.ReLU()
        self.save_hyperparameters()

    def forward(self, x):
        x = self.input(x)
        x = self.relu(x)
        x = self.hidden(x)
        x = self.output(x)
        return x

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x).squeeze(dim=1)
        loss = torch.nn.functional.mse_loss(y_hat, y)
        self.log('train_loss', loss)
        return {'loss': loss}

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x).squeeze(dim=1)
        loss = torch.nn.functional.mse_loss(y_hat, y)
        self.log('val_loss', loss)
        return {'val_loss', loss}

    def test_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x).squeeze(dim=1)
        loss = F.mse_loss(y_hat, y)
        self.log('test_loss', loss)
        return {'test_loss', loss}

    def validation_end(self, validation_step_outputs):
        avg_loss = torch.stack([x['val_loss'] for x in validation_step_outputs]).mean()
        self.log('avg_val_loss', avg_loss)
        return {'avg_val_loss': avg_loss}

    def configure_optimizers(self):
        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)

    def train_dataloader(self):
        return torch.utils.data.DataLoader(train_dataset, batch_size=self.batch_size, num_workers=0)

    def val_dataloader(self):
        return torch.utils.data.DataLoader(val_dataset, batch_size=self.batch_size, num_workers=0)

    def test_dataloader(self):
        return torch.utils.data.DataLoader(test_dataset, batch_size=self.batch_size, num_workers=0)
```

The mathematical notation for the neural network model defined above can be represented as a series of layers, with each layer performing a set of mathematical operations on the input it receives.
The output of one layer serves as the input to the next layer.

The mathematical notation for each hidden layer would be:

$h_i = f(W_i * h_{i-1} + b_i)$

Where:

-   $h_i$ is the output of the i-th hidden layer

-   $h_{i-1}$ is the output of the (i-1)-th hidden layer

-   $W_i$ is the weight matrix for the i-th hidden layer

-   $b_i$ is the bias term for the i-th hidden layer

-   $f$ is the activation function (ReLU)

The mathematical notation for the input layer is:

$h_0 = W_0 * x + b_0$

Where:

-   $h_0$ is the output of the input layer

-   $W_0$ is the weight matrix for the input layer

-   $x$ is the input data

-   $b_0$ is the bias term for the input layer

The mathematical notation for the output layer is:

$y = W_{n+1} * h_n + b_{n+1}$

Where:

-   $y$ is the output variable

-   $h_n$ is the output of the last hidden layer

-   $W_{n+1}$ is the weight matrix for the output layer

-   $b_{n+1}$ is the bias term for the output layer

The optimizer used in the model above is the Adam optimizer.
Adam is an optimization algorithm that combines the benefits of the Adaptive Gradient Algorithm (AdaGrad) and Root Mean Square Propagation (RMSProp).

The mathematical notation for the Adam optimizer is:

$\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t$

Where:

-   $\theta$ is the set of parameters to optimize

-   $\alpha$ is the learning rate

-   $\hat{m}_t$ is the biased first moment estimate

-   $\hat{v}_t$ is the biased second raw moment estimate

-   $\epsilon$ is a small positive constant that is added to the denominator to prevent division by zero

It's worth noting that the Adam optimizer also uses the concepts of momentum and adaptive learning rate which are not reflected in the mathematical notation above.

#### Training the neural network

Next, we want to test the model with predefined parameter set.
Therefore, we are using the **RegressionModel** class defined earlier to create an instance of the model, and then training the model using the **Trainer** class from the **PyTorch Lightning** library.

The **TensorBoardLogger** is used to log the training progress and the results, it is passed to the **Trainer** class and it will save the logs into predefined folder.

The **RegressionModel** instance is created with the following parameters:

-   num_neurons: 64

-   layers: 3

-   learning_rate: 0.001

-   batch_size: 32

The **Trainer** class is initialized with the following parameters:

-   max_epochs: 50

-   logger: the **TensorBoardLogger** created earlier

-   fast_dev_run: False, that means the model will run through the full training process

-   auto_lr_find: True, this will use the learning rate finder to find a good initial learning rate before training.

The fit method is called on the trainer object with the model as its argument, this will start the training process.
The trainer will take care of the training loop, logging progress, and saving the model.

```{python eval=FALSE}
# testing for 64 neurons, 3 layers, learning rate of 0.001, batch size of 32

logger = TensorBoardLogger('lightning_logs', name='insurance')

model = RegressionModel(num_neurons=64, layers=3, learning_rate=0.001, batch_size=32)
#trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=50, logger=logger, fast_dev_run=False, auto_lr_find=True)
trainer = pl.Trainer(max_epochs=50, logger=logger, fast_dev_run=False, auto_lr_find=True)

trainer.fit(model)
```

Before the training, we can check how many weights will be used in the model:

\| Name \| Type \| Params

'--------------------------------------

0 \| input \| Linear \| 448

1 \| hidden \| Sequential \| 12.5 K

2 \| output \| Linear \| 65

3 \| relu \| ReLU \| 0

'--------------------------------------

13.0 K Trainable params

0 Non-trainable params

13.0 K Total params

0.052 Total estimated model params size (MB)

As the result, we obtain the MSE of 34806204 on the train set and MSE of 35508452 on the validation set.

### Hyperparameter Optimization

The HPO process on the neural network model was performed using **Optuna** library in Python.

We let Optuna optimize 4 parameters for us: - 'num_neurons': number of neurons in each layer, between 16 and 256 - 'batch_size': amount of data points used at once, between 32 and 512 - 'layers': amount of layers, between 1 and 5 - 'learning_rate': learning rate of the Adam optimizer, between 1e-5 and 1e-1

```{python eval=FALSE}
def optimize_model(trial):
    
    num_neurons = trial.suggest_int('num_neurons', 16, 256)
    batch_size= trial.suggest_int('batch_size', 32, 512)
    learning_rate= trial.suggest_float('learning_rate', 1e-5, 1e-1)
    layers = trial.suggest_int('layers', 1, 5)

    model = RegressionModel(layers=layers, num_neurons=num_neurons, learning_rate=learning_rate, batch_size=batch_size)
    #trainer = pl.Trainer(accelerator='mps', max_epochs=50)
    trainer = pl.Trainer(max_epochs=50)

    trainer.fit(model)
    return trainer.callback_metrics['val_loss']
    
study = optuna.create_study(direction='minimize', study_name='insurance', storage='sqlite:///insurance.db', load_if_exists=True)
study.optimize(optimize_model, n_trials=100)
```

As the result of the HPO, the the following parameters were found to perform the best on the validation set:

-   'num_neurons': 235
-   'batch_size': 314
-   'layers': 5
-   'learning_rate': 0.0748,

### Performance

As the result, MSE of 18894440 on the validation loss could be achieved, which is almost 50% better than MSE of 35508452 by the predefined model from above.

## Model Comparison

In this chapter, we want a fair comparison of the models.
Therefore, we use the test set, which none of the model have seen yet and predict outcome variable using each model's best parameters, which we determined using HPO.
We compare RMSE and R2 scores for the baseline model, for the regression tree model and for the neural network model.

-   Baseline model:

```{r}
pred.lm.test <- predict(lm1, test, se.fit = TRUE)
RMSE(pred.lm.test$fit, test$charges)
R2(pred.lm.test$fit, test$charges)
```

-   Regression Tree model:

```{r}
pred.regtree.test <- predict(regtree, newdata = test)
RMSE(pred.regtree.test, test$charges)
R2(pred.regtree.test, test$charges)
```

-   Neural Network model:

```{r}
pred.nn.test <- read.csv(file ='./results/predictions_nn.csv', sep = ' ', header = FALSE)
pred.nn.test <- t(pred.nn.test)
pred.nn.test <- head(pred.nn.test, - 1)

RMSE(pred.nn.test, test$charges) 
R2(pred.nn.test, test$charges) 
```

Both of the models could outperform the baseline model, but neural network seems to preform the best.

## Future Work and Discussion
